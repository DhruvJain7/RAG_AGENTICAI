{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6269cb92-f595-498a-acda-673ff749f0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Splitter 1 Statistics ===\n",
      "Total number of chunks: 95\n",
      "Average chunk size: 263.80 characters\n",
      "Metadata keys preserved: page_label, source, moddate, total_pages, creationdate, author, title, producer, page, creator\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): comprehensive support within the field of mental health. \n",
      "Additionally, the paper discusses the implementation of \n",
      "Streamlit to enhance the user ex pe...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 49 characters\n",
      "Max chunk size: 299 characters\n",
      "\n",
      "=== Splitter 2 Statistics ===\n",
      "Total number of chunks: 57\n",
      "Average chunk size: 452.74 characters\n",
      "Metadata keys preserved: page_label, source, moddate, total_pages, creationdate, author, title, producer, page, creator\n",
      "\n",
      "Example chunk:\n",
      "Content (first 150 chars): with severe intellectual disorders do no longer have get entry \n",
      "to the necessary remedy they require. This remedy gap \n",
      "intensifies the weight of intel...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}\n",
      "Min chunk size: 120 characters\n",
      "Max chunk size: 497 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "#Load the LangChain Paper\n",
    "paper_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\"\n",
    "pdf_loader = PyPDFLoader(paper_url)\n",
    "pdf_document = pdf_loader.load()\n",
    "\n",
    "#Load content from LangChain website\n",
    "web_url = \"https://python.langchain.com/v0.2/docs/introduction/\"\n",
    "web_loader = WebBaseLoader(web_url)\n",
    "web_document = web_loader.load()\n",
    "\n",
    "# Create two different text splitters\n",
    "splitter_1 = CharacterTextSplitter(chunk_size=300,chunk_overlap=30,separator=\"\\n\")\n",
    "splitter_2 = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50,separators=[\"\\n\\n\",\"\\n\",\".\",\"\",\"\"])\n",
    "\n",
    "chunks_1 = splitter_1.split_documents(pdf_document)\n",
    "chunks_2 = splitter_2.split_documents(pdf_document)\n",
    "\n",
    "def display_document_stats(docs, name):\n",
    "    \"\"\"Display statistics about a list of document chunks\"\"\"\n",
    "    total_chunks = len(docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    # Count unique metadata keys across all documents\n",
    "    all_metadata_keys = set()\n",
    "    for doc in docs:\n",
    "        all_metadata_keys.update(doc.metadata.keys())\n",
    "    \n",
    "    # Print the statistics\n",
    "    print(f\"\\n=== {name} Statistics ===\")\n",
    "    print(f\"Total number of chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Metadata keys preserved: {', '.join(all_metadata_keys)}\")\n",
    "    \n",
    "    if docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        example_doc = docs[min(5, total_chunks-1)]  # Get the 5th chunk or the last one if fewer\n",
    "        print(f\"Content (first 150 chars): {example_doc.page_content[:150]}...\")\n",
    "        print(f\"Metadata: {example_doc.metadata}\")\n",
    "        \n",
    "        # Calculate length distribution\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        min_len = min(lengths)\n",
    "        max_len = max(lengths)\n",
    "        print(f\"Min chunk size: {min_len} characters\")\n",
    "        print(f\"Max chunk size: {max_len} characters\")\n",
    "\n",
    "# Display stats for both chunk sets\n",
    "display_document_stats(chunks_1, \"Splitter 1\")\n",
    "display_document_stats(chunks_2, \"Splitter 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a003a9e0-88a2-4dc2-8a89-93f40c17ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e06d0da8-a7e1-4d39-b799-d75d9433fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model = \"llama3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a08bcdf-f1a6-431e-897d-26d59cd1f764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008193675, -0.009552899, -0.012233876, 0.0048862672, 0.0035001717]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [text.page_content for text in chunks_1]\n",
    "\n",
    "embedding_result =embeddings.embed_documents(texts)\n",
    "embedding_result[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919925ea-9a8e-4614-bde1-877af9899d10",
   "metadata": {},
   "source": [
    "# Vector stores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9253c66-b9fc-477a-8579-88f5f59e2eca",
   "metadata": {},
   "source": [
    "One of the most common ways to store and search over unstructured data is to embed the text data and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. You can use a [vector store](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/) to store embedded data and perform vector search for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18abcfb6-0553-481c-bdce-65b6c4a7c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29f0679-9920-452a-a5c3-04022b43a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(chunks_1 , embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46a11bdd-71c2-4b19-93f4-b5d688e5ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords —Large Language models , LangChain, Chatbot, \n",
      "Pretrained models, Mental health, Mental health support. \n",
      "I. INTRODUCTION \n",
      "The issue of mental health is an international situation, \n",
      "affecting people in each particularly developed nations and \n",
      "emerging markets. According to the World Health\n"
     ]
    }
   ],
   "source": [
    "query = \"Langchain\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416085e3-3e03-49bd-a782-ed4509e81478",
   "metadata": {},
   "source": [
    "#### Retrievers\n",
    "A retriever is an interface that returns documents using an unstructured query. Retrievers are more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. You can still use vector stores as the backbone of a retriever. Note that other types of retrievers also exist.\n",
    "\n",
    "Retrievers accept a string query as input and return a list of Documents as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e276ff4-b659-4725-ad91-4ccd6418a944",
   "metadata": {},
   "source": [
    "##### **Vector store-backed retrievers** \n",
    "Vector store retrievers are retrievers that use a vector store to retrieve documents. They are a lightweight wrapper around the vector store class to make it conform to the retriever interface. They use the search methods implemented by a vector store, such as similarity search and MMR (Maximum marginal relevance), to query the texts in the vector store.\n",
    "\n",
    "Now that you have constructed a vector store docsearch, you can easily construct a retriever such as seen in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45dd42d-592a-4c08-8623-6cdd231233b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords —Large Language models , LangChain, Chatbot, \n",
      "Pretrained models, Mental health, Mental health support. \n",
      "I. INTRODUCTION \n",
      "The issue of mental health is an international situation, \n",
      "affecting people in each particularly developed nations and \n",
      "emerging markets. According to the World Health\n"
     ]
    }
   ],
   "source": [
    "# Use the docsearch vector store as a retriever\n",
    "# This converts the vector store into a retriever interface that can fetch relevant documents\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "# Invoke the retriever with the query \"Langchain\"\n",
    "# This will:\n",
    "# 1. Convert the query text \"Langchain\" into an embedding vector\n",
    "# 2. Perform a similarity search in the vector store using this embedding\n",
    "# 3. Return the most semantically similar documents to the query\n",
    "docs = retriever.invoke(\"Langchain\")\n",
    "# Access the first (most relevant) document from the retrieval results\n",
    "# This returns the full Document object including:\n",
    "# - page_content: The text content of the document\n",
    "# - metadata: Any associated metadata like source, page numbers, etc.\n",
    "# The returned document is the one most semantically similar to \"Langchain\"\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e0c27-421c-49d0-ae7c-38020a299ff2",
   "metadata": {},
   "source": [
    "##### **Parent document retrievers**\n",
    "When splitting documents for retrieval, there are often conflicting goals:\n",
    "\n",
    "- You want small documents so their embeddings can most accurately reflect their meaning. If the documents are too long, then the embeddings can lose meaning.\n",
    "- You want to have long enough documents to retain the context of each chunk of text.\n",
    "\n",
    "The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. During retrieval, this retriever first fetches the small chunks, but then looks up the parent IDs for the data and returns those larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3c59e4-a97c-4bd9-8c14-b8accc1e0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers import ParentDocumentRetriever\n",
    "from langchain_classic.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "961d4de5-5f00-4988-bcb1-b013aa275e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up two different text splitters for a hierarchical splitting approach:\n",
    "\n",
    "# 1. Parent splitter creates larger chunks (2000 characters)\n",
    "# This is used to split documents into larger, more contextually complete sections\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\\n')\n",
    "\n",
    "# 2. Child splitter creates smaller chunks (400 characters)\n",
    "# This is used to split the parent chunks into smaller pieces for more precise retrieval\n",
    "child_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20, separator='\\n')\n",
    "\n",
    "# Create a Chroma vector store with:\n",
    "# - A specific collection name \"split_parents\" for organization\n",
    "# - The previously configured Watson embeddings function\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Set up an in-memory storage layer for the parent documents\n",
    "# This will store the larger chunks that provide context, but won't be directly embedded\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5865973-bb95-4e10-85aa-4f0653bbe24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ParentDocumentRetriever instance that implements hierarchical document retrieval\n",
    "retriever = ParentDocumentRetriever(\n",
    "    # The vector store where child document embeddings will be stored and searched\n",
    "    # This Chroma instance will contain the embeddings for the smaller chunks\n",
    "    vectorstore=vectorstore,\n",
    "    \n",
    "    # The document store where parent documents will be stored\n",
    "    # These larger chunks won't be embedded but will be retrieved by ID when needed\n",
    "    docstore=store,\n",
    "    \n",
    "    # The splitter used to create small chunks (400 chars) for precise vector search\n",
    "    # These smaller chunks are embedded and used for similarity matching\n",
    "    child_splitter=child_splitter,\n",
    "    \n",
    "    # The splitter used to create larger chunks (2000 chars) for better context\n",
    "    # These parent chunks provide more complete information when retrieved\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a53892a-cd6e-4f7e-b03b-81673ce5980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(pdf_document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36d17447-8b14-489f-a7df-bf2807343b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50cea34d-a25a-4d6f-a1d2-6238d6634fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ed7d9ca-e8ae-44c7-8371-1c6cbb931764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and the chatbot's responses, allowing for a \n",
      "dynamic and coherent conversation flow. \n",
      "• Chatmodel Class of LangChain: The LangChain\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eb4f97a-61e4-4274-8451-879138114bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78c900ab-c171-4042-bb58-8a2f40c35a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "their mental health questions, kicking off a series of \n",
      "interactions with the LangChain framework. This is where the \n",
      "magic happens – LangChain acts as the brain behind the \n",
      "chatbot, working through various components like chat \n",
      "message templates and a memory concept to create a \n",
      "personalized and responsive support system.  Each step is \n",
      "broken down. \n",
      "Step 1. User Interface:  Developed using the Streamlit \n",
      "framework, the user interface welcomes users with a \n",
      "message explaining the role of the chatbot in providing \n",
      "mental health support. It assures users of a safe and \n",
      "confidential space to express their concerns.  \n",
      "Step 2. User Input - Prompt: Users can input mental health-\n",
      "related questions or seek advice by typing their queries \n",
      "into the input box integrated into the Streamlit interface. \n",
      "Step 3. Data Transfer to LangChain: Implement the \n",
      "functionality that sends the user's input (question) as a \n",
      "chat prompt template to the LangChain framework. This \n",
      "input serves as the \"human message prompt\" template. \n",
      "Step 4. LangChain Framework: In this phase, the LangChain \n",
      "framework serves as the backbone of the chatbot, where \n",
      "all the foundational components and building blocks are \n",
      "meticulously orchestrated. Here's a deeper dive into the \n",
      "critical elements of LangChain Processing: \n",
      "• ChatMessage and Prompt Templates:  Within \n",
      "LangChain, the chatbot's core communication \n",
      "infrastructure is established by  creating \n",
      "ChatMessage and prompt templates for optimal \n",
      "chatbot engagement. \n",
      "• LLMChain and LLM Model Interaction:  To \n",
      "facilitate interactions with the large language \n",
      "model (LLM), a specialized component called \n",
      "LLMChain is constructed. The LLMChain acts \n",
      "as a conduit for managing the flow of \n",
      "conversation between the chatbot and the LLM \n",
      "model, in this case, GPT-4. \n",
      "• The LLMChain handles both the user's queries \n",
      "and the chatbot's responses, allowing for a \n",
      "dynamic and coherent conversation flow. \n",
      "• Chatmodel Class of LangChain: The LangChain\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35f412-d173-4d95-9902-dec8ffdd026c",
   "metadata": {},
   "source": [
    "##### **RetrievalQA** \n",
    "\n",
    "Now that you understand how to retrieve information from a document, you might be interested in exploring some more exciting applications. For instance, you could have the Language Model (LLM) read the paper and summarize it for you, or create a QA bot that can answer your questions based on the paper.\n",
    "\n",
    "Here's an example using LangChain's `RetrievalQA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6e824ad-8c6a-4f4a-8951-c7975068e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama \n",
    "\n",
    "chat_llm = ChatOllama(\n",
    "    model = \"llama3\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcdd0217-ea5b-49a6-b4c2-97c64162867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8828fd76-a65b-4171-945c-9fe89f99edc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is this paper discussing?',\n",
       " 'result': 'This paper appears to be discussing mental health challenges, particularly suicide attempts and the remedies that are lacking for individuals struggling with these issues. It touches on the idea that there is a gap in the assistance and care available for people dealing with intellectual health troubles, which can exacerbate their struggles.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RetrievalQA chain by configuring:\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    #The language model to use for generating answers\n",
    "    llm = chat_llm,\n",
    "    # The chain type \"stuff\" means all retrieved documents are simply concatenated and passed to the LLM\n",
    "    \n",
    "    chain_type = \"stuff\",\n",
    "    # The retriever component that will fetch relevant documents\n",
    "    # docsearch.as_retriever() converts the vector store into a retriever interface\n",
    "    retriever = docsearch.as_retriever(),\n",
    "    # Whether to include the source documents in the response\n",
    "    # Set to False to return only the generated answer\n",
    "    return_source_documents = False\n",
    "    \n",
    ")\n",
    "query = \"What is this paper discussing?\"\n",
    "\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7dc11e-6e7d-4105-988f-e1a6a9b21bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
